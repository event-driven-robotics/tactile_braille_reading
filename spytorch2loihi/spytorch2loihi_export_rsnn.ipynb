{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys, os\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import h5py\n",
    "import torch\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "from SlayerSNN_src.auto.loihi import denseBlock, convBlock, flattenBlock, poolBlock, Network\n",
    "from SlayerSNN_src.slayerLoihi import spikeLayer as loihi\n",
    "from SlayerSNN_src import utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from IPython.display import HTML\n",
    "\n",
    "from SlayerSNN_src.slayer import spikeLayer as layer\n",
    "from SlayerSNN_src.slayerLoihi import spikeLayer as loihi\n",
    "from SlayerSNN_src.slayerParams import yamlParams as params\n",
    "from SlayerSNN_src.spikeLoss import spikeLoss as loss\n",
    "from SlayerSNN_src.spikeClassifier import spikeClassifier as predict\n",
    "from SlayerSNN_src import spikeFileIO as io\n",
    "from SlayerSNN_src import utils\n",
    "# This will be removed later. Kept for compatibility only\n",
    "from SlayerSNN_src.quantizeParams import quantizeWeights as quantize\n",
    "\n",
    "# Added for debug\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold\n",
    "threshold = 10 # 1, 2, 5, 10\n",
    "run = \"_3\"\n",
    "\n",
    "if threshold == 1:\n",
    "    time_bin_size = 5\n",
    "    nb_input_copies = 2\n",
    "    tau_mem = 0.06\n",
    "    tau_ratio = 10\n",
    "elif threshold == 2:\n",
    "    time_bin_size = 3\n",
    "    nb_input_copies = 8\n",
    "    tau_mem = 0.05\n",
    "    tau_ratio = 10\n",
    "elif threshold == 5:\n",
    "    time_bin_size = 3\n",
    "    nb_input_copies = 4\n",
    "    tau_mem = 0.07\n",
    "    tau_ratio = 10\n",
    "elif threshold == 10:\n",
    "    time_bin_size = 5\n",
    "    nb_input_copies = 2\n",
    "    tau_mem = 0.07\n",
    "    tau_ratio = 10\n",
    "\n",
    "# SpyTorch weights\n",
    "weights_path = \"weights/SpyTorch_trained_weights_rec_th\" + str(threshold) + run + \".pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import weights\n",
    "path = weights_path\n",
    "SpyTorch_weights = torch.load(path, map_location=torch.device('cpu'))\n",
    "\n",
    "wgt1_in2hid = SpyTorch_weights[0].detach().numpy()\n",
    "wgt2_hid2out = SpyTorch_weights[1].detach().numpy()\n",
    "wgt3_hid2hid = SpyTorch_weights[2].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(450, 28)\n",
      "[[-0.06104714  0.19783527 -0.23018488 ...  0.21285294  0.0506044\n",
      "  -0.82487   ]\n",
      " [-0.29335156 -0.1839731  -0.12959073 ... -0.07209709 -0.02327315\n",
      "  -0.9899565 ]\n",
      " [-0.27198118 -0.29631394  0.02032759 ... -0.3200279  -0.02387381\n",
      "  -1.4972667 ]\n",
      " ...\n",
      " [-0.00854561 -0.3378115  -0.19617951 ... -0.00971493  0.27904868\n",
      "  -0.24223976]\n",
      " [-0.38971943 -0.1552923  -0.07672878 ... -0.0064165   0.28447646\n",
      "  -1.5011324 ]\n",
      " [-0.12439619 -0.04238946 -0.06246538 ...  0.2522859  -0.25524408\n",
      "  -1.2072037 ]]\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(SpyTorch_weights[1].detach().numpy()))\n",
    "print(SpyTorch_weights[1].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48, 450)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(SpyTorch_weights[0].detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wgt_scale_calc =  70\n",
      "vDecay_calc =  282\n",
      "iDecay_calc =  2090\n",
      "time_bins =  270\n"
     ]
    }
   ],
   "source": [
    "# Loihi inference parameters approximation\n",
    "wgt_max = np.amax([np.amax(np.abs(wgt1_in2hid)), \n",
    "                   np.amax(np.abs(wgt2_hid2out)), \n",
    "                   np.amax(np.abs(wgt3_hid2hid))])\n",
    "wgt_scale_calc = math.floor(256/wgt_max) # round down\n",
    "tau_syn = tau_mem/tau_ratio\n",
    "alpha = float(np.exp(-(time_bin_size/1000)/tau_syn))\n",
    "beta = float(np.exp(-(time_bin_size/1000)/tau_mem))\n",
    "vDecay_calc = int(4096-4096*beta)\n",
    "iDecay_calc = int(4096-4096*alpha)\n",
    "time_bins = math.ceil(1350/time_bin_size) # round up\n",
    "\n",
    "print(\"wgt_scale_calc = \", wgt_scale_calc)\n",
    "print(\"vDecay_calc = \", vDecay_calc)\n",
    "print(\"iDecay_calc = \", iDecay_calc)\n",
    "print(\"time_bins = \", time_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loihi inference parameters\n",
    "wgt_scale = wgt_scale_calc # scale for all weight\n",
    "vThMant = wgt_scale_calc # vth = vthMant âˆ— 64\n",
    "vDecay = vDecay_calc # tau_mem\n",
    "iDecay = iDecay_calc # tau_syn\n",
    "\n",
    "qtz_step = 2 # weights quantization step\n",
    "rec_scale = 1 # extra scale for recurrent weights\n",
    "refDelay = 1 # refractory delay\n",
    "wgtExp = 0 # 2**(6+wgtExp) * W * spike_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.20000e+01, 3.30000e+01, 1.21000e+02, 2.91000e+02, 7.45000e+02,\n",
       "        1.64700e+03, 7.60700e+03, 1.79059e+05, 1.29340e+04, 5.10000e+01]),\n",
       " array([-79.978546 , -69.50476  , -59.03097  , -48.557182 , -38.083397 ,\n",
       "        -27.609608 , -17.13582  ,  -6.6620336,   3.8117537,  14.285542 ,\n",
       "         24.759329 ], dtype=float32),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(SpyTorch_weights[2].detach().numpy().flatten() * wgt_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape weights\n",
    "spy_weights = []\n",
    "\n",
    "spy_weights.append([]) # flatten layer\n",
    "\n",
    "spy_weights.append(np.reshape(\n",
    "                        np.transpose(\n",
    "                            SpyTorch_weights[0].detach().numpy()\n",
    "                        ), \n",
    "                        (450, 24*nb_input_copies, 1, 1, 1)) * wgt_scale)\n",
    "\n",
    "spy_weights.append(np.reshape(\n",
    "                        np.transpose(\n",
    "                            SpyTorch_weights[1].detach().numpy()\n",
    "                        ),\n",
    "                        (28, 450, 1, 1, 1)) * wgt_scale)\n",
    "\n",
    "spy_recWeights = np.reshape(\n",
    "                        np.transpose(\n",
    "                            SpyTorch_weights[2].detach().numpy()\n",
    "                        ),\n",
    "                        (450, 450, 1, 1, 1)) * wgt_scale * rec_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe the network\n",
    "netDesc = {\n",
    "    'simulation' : {\n",
    "        'Ts': 1,\n",
    "        'tSample': time_bins,\n",
    "    },\n",
    "    'neuron' : {\n",
    "        'type'     : 'LOIHI',\n",
    "        'vThMant'  : vThMant,\n",
    "        'vDecay'   : vDecay,\n",
    "        'iDecay'   : iDecay,\n",
    "        'refDelay' : refDelay,\n",
    "        'wgtExp'   : wgtExp,\n",
    "        'tauRho'   : 1, # useless in inference\n",
    "        'scaleRho' : 1, # useless in inference\n",
    "    },\n",
    "    'layer' : [\n",
    "        {'dim' : \"'\" + str(24*nb_input_copies) + \"x1x1\"}, # Width x Height x Channels\n",
    "        {'dim' : '450r', 'delay' : False},\n",
    "        {'dim' : 28, 'delay' : False}\n",
    "    ]\n",
    "}\n",
    "\n",
    "netParams = params(dict=netDesc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class recurrentBlock(torch.nn.Module):\n",
    "    def __init__(self, slayer, inFeatures, outFeatures, weightScale, \n",
    "                 preHookFx = lambda x: utils.quantize(x, step=qtz_step), weightNorm=False, \n",
    "                 delay=False, maxDelay=62, countLog=False):\n",
    "        super(recurrentBlock, self).__init__()\n",
    "        self.slayer = slayer\n",
    "        self.weightNorm = weightNorm\n",
    "        if weightNorm is True:\n",
    "            self.weightOp = torch.nn.utils.weight_norm(slayer.dense(\n",
    "                inFeatures, outFeatures, weightScale, preHookFx), name='weight')\n",
    "            self.recWeightOp = torch.nn.utils.weight_norm(slayer.dense(\n",
    "                outFeatures, outFeatures, weightScale, preHookFx), name='recWeight')\n",
    "        else:\n",
    "            self.weightOp = slayer.dense(inFeatures, outFeatures, weightScale, preHookFx)\n",
    "            self.recWeightOp = slayer.dense(outFeatures, outFeatures, weightScale, preHookFx)\n",
    "        self.delayOp  = slayer.delay(outFeatures) if delay is True else None\n",
    "        self.countLog = countLog\n",
    "        self.gradLog = True\n",
    "        self.maxDelay = maxDelay\n",
    "        \n",
    "        self.paramsDict = {\n",
    "            'inFeatures'  : inFeatures,\n",
    "            'outFeatures' : outFeatures,\n",
    "        }\n",
    "    \n",
    "    def forward(self, spike):\n",
    "        spike = self.slayer.spikeLoihi(self.weightOp(spike) + self.recWeightOp(spike))\n",
    "        spike = self.slayer.delayShift(spike, 1)\n",
    "        if self.delayOp is not None:\n",
    "            spike = self.delayOp(spike)\n",
    "        if self.countLog is True:\n",
    "            return spike, torch.sum(spike)\n",
    "        else:\n",
    "            return spike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpyTorch2Loihi(Network):\n",
    "    \n",
    "    def __init__(self, netParams, weights, recWeights):\n",
    "        super(SpyTorch2Loihi, self).__init__(netParams)\n",
    "        self.preHookFx = lambda x: utils.quantize(x, step=qtz_step)\n",
    "        \n",
    "        # Load the weights trained in spytorch\n",
    "        self.weights = weights\n",
    "        self.recWeights = recWeights\n",
    "    \n",
    "    \n",
    "    def _layerType(self, dim):\n",
    "        if type(dim) is int:\n",
    "            return 'dense'\n",
    "        elif dim.find('c') != -1:\n",
    "            return 'conv'\n",
    "        elif dim.find('avg') != -1:\n",
    "            return 'average'\n",
    "        elif dim.find('a') != -1:\n",
    "            return 'pool'\n",
    "        elif dim.find('x') != -1:\n",
    "            return 'input'\n",
    "        elif dim.find('r') != -1:\n",
    "            return 'recurrent'\n",
    "        else:\n",
    "            raise Exception('Could not parse the layer description. Found {}'.format(dim))\n",
    "        # return [int(i) for i in re.findall(r'\\d+', dim)]\n",
    "\n",
    "    \n",
    "    def _parseLayers(self):\n",
    "        i = 0\n",
    "        blocks = torch.nn.ModuleList()\n",
    "        layerDim = [] # CHW\n",
    "        is1Dconv = False\n",
    "\n",
    "        print('\\nNetwork Architecture:')\n",
    "        # print('=====================')\n",
    "        print(self._tableStr(header=True))\n",
    "\n",
    "        for layer in self.netParams['layer']:\n",
    "            layerType = self._layerType(layer['dim'])\n",
    "            # print(i, layerType)\n",
    "\n",
    "            # If layer has neuron feild, then use the slayer initialized with it and self.netParams['simulation']\n",
    "            if 'neuron' in layer.keys():\n",
    "                print(layerType, 'using individual slayer')\n",
    "                slayer = loihi(layer['neuron'], self.netParams['simulation'])\n",
    "            else:\n",
    "                slayer = self.slayer\n",
    "\n",
    "            if i==0 and self.inputShape is None: \n",
    "                if layerType == 'input':\n",
    "                    self.inputShape = tuple([int(numStr) for numStr in re.findall(r'\\d+', layer['dim'])])\n",
    "                    if len(self.inputShape) == 3:\n",
    "                        layerDim = list(self.inputShape)[::-1]\n",
    "                    elif len(self.inputShape) == 2:\n",
    "                        layerDim = [1, self.inputShape[1], self.inputShape[0]]\n",
    "                    else:\n",
    "                        raise Exception('Could not parse the input dimension. Got {}'.format(self.inputShape))\n",
    "                elif layerType == 'dense':\n",
    "                    self.inputShape = tuple([layer['dim']])\n",
    "                    layerDim = [layer['dim'], 1, 1]\n",
    "                else:\n",
    "                    raise Exception('Input dimension could not be determined! It should be the first entry in the' \n",
    "                                    + \"'layer' feild.\")\n",
    "                # print(self.inputShape)\n",
    "                print(self._tableStr('Input', layerDim[2], layerDim[1], layerDim[0]))\n",
    "                if layerDim[1] == 1:\n",
    "                    is1Dconv = True\n",
    "            else:\n",
    "                # print(i, layer['dim'], self._layerType(layer['dim']))\n",
    "                if layerType == 'conv':\n",
    "                    params = [int(i) for i in re.findall(r'\\d+', layer['dim'])]\n",
    "                    inChannels  = layerDim[0]\n",
    "                    outChannels = params[0]\n",
    "                    kernelSize  = params[1]\n",
    "                    stride      = layer['stride']   if 'stride'   in layer.keys() else 1\n",
    "                    padding     = layer['padding']  if 'padding'  in layer.keys() else kernelSize//2\n",
    "                    dilation    = layer['dilation'] if 'dilation' in layer.keys() else 1\n",
    "                    groups      = layer['groups']   if 'groups'   in layer.keys() else 1\n",
    "                    weightScale = layer['wScale']   if 'wScale'   in layer.keys() else 100\n",
    "                    delay       = layer['delay']    if 'delay'    in layer.keys() else False\n",
    "                    maxDelay    = layer['maxDelay'] if 'maxDelay' in layer.keys() else 62\n",
    "                    # print(i, inChannels, outChannels, kernelSize, stride, padding, dilation, groups, weightScale)\n",
    "                    \n",
    "                    if is1Dconv is False:\n",
    "                        blocks.append(convBlock(slayer, inChannels, outChannels, kernelSize, stride, padding, \n",
    "                                                dilation, groups, weightScale, self.preHookFx, self.weightNorm, \n",
    "                                                delay, maxDelay, self.countLog))\n",
    "                        layerDim[0] = outChannels\n",
    "                        layerDim[1] = int(np.floor((layerDim[1] + 2*padding - dilation * (kernelSize - 1) - 1)/stride + 1))\n",
    "                        layerDim[2] = int(np.floor((layerDim[2] + 2*padding - dilation * (kernelSize - 1) - 1)/stride + 1))\n",
    "                    else:\n",
    "                        blocks.append(convBlock(slayer, inChannels, outChannels, [1, kernelSize], [1, stride], [0, padding], \n",
    "                                                [1, dilation], groups, weightScale, self.preHookFx, self.weightNorm, \n",
    "                                                delay, maxDelay, self.countLog))\n",
    "                        layerDim[0] = outChannels\n",
    "                        layerDim[1] = 1\n",
    "                        layerDim[2] = int(np.floor((layerDim[2] + 2*padding - dilation * (kernelSize - 1) - 1)/stride + 1))\n",
    "                    self.layerDims.append(layerDim.copy())\n",
    "\n",
    "                    print(self._tableStr('Conv', layerDim[2], layerDim[1], layerDim[0], kernelSize, stride, padding, \n",
    "                          delay, sum(p.numel() for p in blocks[-1].parameters() if p.requires_grad)))\n",
    "                elif layerType == 'pool':\n",
    "                    params = [int(i) for i in re.findall(r'\\d+', layer['dim'])]\n",
    "                    # print(params[0])\n",
    "                    \n",
    "                    blocks.append(poolBlock(slayer, params[0], countLog=self.countLog))\n",
    "                    layerDim[1] = int(np.ceil(layerDim[1] / params[0]))\n",
    "                    layerDim[2] = int(np.ceil(layerDim[2] / params[0]))\n",
    "                    self.layerDims.append(layerDim.copy())\n",
    "\n",
    "                    print(self._tableStr('Pool', layerDim[2], layerDim[1], layerDim[0], params[0]))\n",
    "                elif layerType == 'dense':\n",
    "                    params = layer['dim']\n",
    "                    # print(params)\n",
    "                    if layerDim[1] != 1 or layerDim[2] != 1: # needs flattening of layers\n",
    "                        blocks.append(flattenBlock(self.countLog ))\n",
    "                        layerDim[0] = layerDim[0] * layerDim[1] * layerDim[2]\n",
    "                        layerDim[1] = layerDim[2] = 1\n",
    "                        self.layerDims.append(layerDim.copy())\n",
    "                    weightScale = layer['wScale']   if 'wScale'   in layer.keys() else 100\n",
    "                    delay       = layer['delay']    if 'delay'    in layer.keys() else False\n",
    "                    maxDelay    = layer['maxDelay'] if 'maxDelay' in layer.keys() else 62\n",
    "                    \n",
    "                    blocks.append(denseBlock(slayer, layerDim[0], params, weightScale, self.preHookFx, \n",
    "                                  self.weightNorm, delay, maxDelay, self.countLog))\n",
    "                    layerDim[0] = params\n",
    "                    layerDim[1] = layerDim[2] = 1\n",
    "                    self.layerDims.append(layerDim.copy())\n",
    "\n",
    "                    print(self._tableStr('Dense', layerDim[2], layerDim[1], layerDim[0], delay=delay, \n",
    "                                        numParams=sum(p.numel() for p in blocks[-1].parameters() if p.requires_grad)))\n",
    "                elif layerType == 'recurrent':\n",
    "                    #params = layer['dim']\n",
    "                    params = [int(i) for i in re.findall(r'\\d+', layer['dim'])]\n",
    "                    # print(params)\n",
    "                    if layerDim[1] != 1 or layerDim[2] != 1: # needs flattening of layers\n",
    "                        blocks.append(flattenBlock(self.countLog ))\n",
    "                        layerDim[0] = layerDim[0] * layerDim[1] * layerDim[2]\n",
    "                        layerDim[1] = layerDim[2] = 1\n",
    "                        self.layerDims.append(layerDim.copy())\n",
    "                    weightScale = layer['wScale']   if 'wScale'   in layer.keys() else 100\n",
    "                    delay       = layer['delay']    if 'delay'    in layer.keys() else False\n",
    "                    maxDelay    = layer['maxDelay'] if 'maxDelay' in layer.keys() else 62\n",
    "                    \n",
    "                    blocks.append(recurrentBlock(slayer, layerDim[0], params[0], weightScale, self.preHookFx, \n",
    "                                  self.weightNorm, delay, maxDelay, self.countLog))\n",
    "                    layerDim[0] = params[0]\n",
    "                    layerDim[1] = layerDim[2] = 1\n",
    "                    self.layerDims.append(layerDim.copy())\n",
    "\n",
    "                    print(self._tableStr('Recurrent', layerDim[2], layerDim[1], layerDim[0], delay=delay, \n",
    "                                        numParams=sum(p.numel() for p in blocks[-1].parameters() if p.requires_grad)))\n",
    "                elif layerType == 'average':\n",
    "                    params = [int(i) for i in re.findall(r'\\d+', layer['dim'])]\n",
    "                    layerDim[0] = params[0]\n",
    "                    layerDim[1] = layerDim[2] = 1\n",
    "                    self.layerDims.append(layerDim.copy())\n",
    "\n",
    "                    blocks.append(averageBlock(nOutputs=layerDim[0], countLog=self.countLog))\n",
    "                    print(self._tableStr('Average', 1, 1, params[0]))\n",
    "\n",
    "                i += 1\n",
    "        self.nOutput = layerDim[0] * layerDim[1] * layerDim[2]\n",
    "        print(self._tableStr(numParams=sum(p.numel() for p in blocks.parameters() if p.requires_grad), footer=True))\n",
    "        return blocks\n",
    "    \n",
    "    \n",
    "    def genSpyModel(self, fname):\n",
    "        qWeights = lambda x: self.preHookFx(x).cpu().data.numpy().squeeze()\n",
    "\n",
    "        h = h5py.File(fname, 'w')\n",
    "\n",
    "        simulation = h.create_group('simulation')\n",
    "\n",
    "        for key, value in self.netParams['simulation'].items():\n",
    "            # print(key, value)\n",
    "            simulation[key] = value\n",
    "\n",
    "        layer = h.create_group('layer')\n",
    "        layer.create_dataset('0/type', (1, ), 'S10', [b'input'])\n",
    "        layer.create_dataset('0/shape', data=np.array([self.inputShape[2], self.inputShape[1], self.inputShape[0]]))\n",
    "        \n",
    "        for i, block in enumerate(self.blocks):\n",
    "            print(\"\\nblock %d / %d\" % (i, len(self.blocks)))\n",
    "            layerType = block.__class__.__name__[:-5]\n",
    "            \n",
    "            print(layerType.encode('ascii', 'ignore'))\n",
    "            layer.create_dataset('{}/type'.format(i+1), (1, ), 'S10', [layerType.encode('ascii', 'ignore')])\n",
    "            \n",
    "            print(i, self.layerDims[i])\n",
    "            layer.create_dataset('{}/shape'.format(i+1), data=np.array(self.layerDims[i]))\n",
    "            \n",
    "            if layerType != 'flatten':\n",
    "                layer.create_dataset('{}/weight'.format(i+1), data=qWeights(torch.Tensor(self.weights[i])))\n",
    "                if layerType == 'recurrent':\n",
    "                    layer.create_dataset('{}/recWeight'.format(i+1), data=qWeights(torch.Tensor(self.recWeights)))\n",
    "                    \n",
    "            for key, param in block.paramsDict.items():\n",
    "                layer.create_dataset('{}/{}'.format(i+1, key), data=param)\n",
    "                \n",
    "            if layerType != 'flatten' and layerType != 'average':\n",
    "                for key, value in block.slayer.neuron.items():\n",
    "                    # print(i, key, value)\n",
    "                    layer.create_dataset('{}/neuron/{}'.format(i+1, key), data=value)\n",
    "        h.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simulation:\n",
      "    Ts         : 1\n",
      "    tSample    : 270\n",
      "\n",
      "neuron:\n",
      "    type       : LOIHI\n",
      "    vThMant    : 70\n",
      "    vDecay     : 282\n",
      "    iDecay     : 2090\n",
      "    refDelay   : 1\n",
      "    wgtExp     : 0\n",
      "    tauRho     : 1\n",
      "    scaleRho   : 1\n",
      "\n",
      "Max PSP kernel: 99.0\n",
      "Scaling neuron[scaleRho] by Max PSP Kernel @slayerLoihi\n",
      "\n",
      "Network Architecture:\n",
      "|   Type   |  W  |  H  |  C  | ker | str | pad |delay|  params  |\n",
      "|Input     |   48|    1|    1|     |     |     |False|          |\n",
      "|Recurrent |    1|    1|  450|     |     |     |False|    224100|\n",
      "|Dense     |    1|    1|   28|     |     |     |False|     12600|\n",
      "|Total                                               |    236700|\n",
      "TODO core usage estimator\n"
     ]
    }
   ],
   "source": [
    "# Create the network\n",
    "netLoihi = SpyTorch2Loihi(netParams, spy_weights, spy_recWeights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "block 0 / 3\n",
      "b'flatten'\n",
      "0 [48, 1, 1]\n",
      "\n",
      "block 1 / 3\n",
      "b'recurrent'\n",
      "1 [450, 1, 1]\n",
      "\n",
      "block 2 / 3\n",
      "b'dense'\n",
      "2 [28, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Export the model\n",
    "netLoihi.genSpyModel('netsLoihi/netLoihi_rec_th' + str(threshold) + run + '.net')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9bf43d2b4a4b64acce80ec436e45972ca3a2814376cdde651dbbddec46144e4b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('pyenv_pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
